{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fLkkTLx0riTY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.3.27-py3-none-any.whl.metadata (7.8 kB)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.6.5-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting langchain_core\n",
            "  Using cached langchain_core-0.3.74-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.3.30-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting langchain-text-splitters<1.0.0,>=0.3.9 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.9-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting langsmith>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.4.14-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting pydantic<3.0.0,>=2.7.4 (from langchain)\n",
            "  Downloading pydantic-2.11.7-py3-none-any.whl.metadata (67 kB)\n",
            "Collecting SQLAlchemy<3,>=1.4 (from langchain)\n",
            "  Downloading sqlalchemy-2.0.43-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.6 kB)\n",
            "Collecting requests<3,>=2 (from langchain)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting PyYAML>=5.3 (from langchain)\n",
            "  Using cached PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
            "Collecting langgraph-checkpoint<3.0.0,>=2.1.0 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langgraph-prebuilt<0.7.0,>=0.6.0 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.6.4-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting langgraph-sdk<0.3.0,>=0.2.0 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.2.2-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting xxhash>=3.5.0 (from langgraph)\n",
            "  Downloading xxhash-3.5.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
            "Collecting tenacity!=8.4.0,<10.0.0,>=8.1.0 (from langchain_core)\n",
            "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain_core)\n",
            "  Using cached jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting typing-extensions>=4.7 (from langchain_core)\n",
            "  Downloading typing_extensions-4.14.1-py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging>=23.2 in ./.venv/lib/python3.12/site-packages (from langchain_core) (25.0)\n",
            "Collecting openai<2.0.0,>=1.99.9 (from langchain_openai)\n",
            "  Downloading openai-1.100.2-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain_openai)\n",
            "  Downloading tiktoken-0.11.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain_core)\n",
            "  Using cached jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint<3.0.0,>=2.1.0->langgraph)\n",
            "  Downloading ormsgpack-1.10.0-cp312-cp312-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl.metadata (43 kB)\n",
            "Collecting httpx>=0.25.2 (from langgraph-sdk<0.3.0,>=0.2.0->langgraph)\n",
            "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting orjson>=3.10.1 (from langgraph-sdk<0.3.0,>=0.2.0->langgraph)\n",
            "  Downloading orjson-3.11.2-cp312-cp312-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (1.2 kB)\n",
            "Collecting requests-toolbelt>=1.0.0 (from langsmith>=0.1.17->langchain)\n",
            "  Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Collecting zstandard>=0.23.0 (from langsmith>=0.1.17->langchain)\n",
            "  Downloading zstandard-0.24.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.1 kB)\n",
            "Collecting anyio<5,>=3.5.0 (from openai<2.0.0,>=1.99.9->langchain_openai)\n",
            "  Downloading anyio-4.10.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Collecting distro<2,>=1.7.0 (from openai<2.0.0,>=1.99.9->langchain_openai)\n",
            "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.99.9->langchain_openai)\n",
            "  Downloading jiter-0.10.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
            "Collecting sniffio (from openai<2.0.0,>=1.99.9->langchain_openai)\n",
            "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting tqdm>4 (from openai<2.0.0,>=1.99.9->langchain_openai)\n",
            "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
            "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.33.2 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
            "  Downloading pydantic_core-2.33.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
            "Collecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.7.4->langchain)\n",
            "  Downloading typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting charset_normalizer<4,>=2 (from requests<3,>=2->langchain)\n",
            "  Downloading charset_normalizer-3.4.3-cp312-cp312-macosx_10_13_universal2.whl.metadata (36 kB)\n",
            "Collecting idna<4,>=2.5 (from requests<3,>=2->langchain)\n",
            "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2->langchain)\n",
            "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests<3,>=2->langchain)\n",
            "  Downloading certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting regex>=2022.1.18 (from tiktoken<1,>=0.7->langchain_openai)\n",
            "  Downloading regex-2025.7.34-cp312-cp312-macosx_11_0_arm64.whl.metadata (40 kB)\n",
            "Collecting httpcore==1.* (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph)\n",
            "  Downloading httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11>=0.16 (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.0->langgraph)\n",
            "  Downloading h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
            "Downloading langchain-0.3.27-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph-0.6.5-py3-none-any.whl (153 kB)\n",
            "Using cached langchain_core-0.3.74-py3-none-any.whl (443 kB)\n",
            "Downloading langchain_openai-0.3.30-py3-none-any.whl (74 kB)\n",
            "Using cached jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading langchain_text_splitters-0.3.9-py3-none-any.whl (33 kB)\n",
            "Downloading langgraph_checkpoint-2.1.1-py3-none-any.whl (43 kB)\n",
            "Downloading langgraph_prebuilt-0.6.4-py3-none-any.whl (28 kB)\n",
            "Downloading langgraph_sdk-0.2.2-py3-none-any.whl (52 kB)\n",
            "Downloading langsmith-0.4.14-py3-none-any.whl (373 kB)\n",
            "Downloading openai-1.100.2-py3-none-any.whl (787 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m787.8/787.8 kB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic-2.11.7-py3-none-any.whl (444 kB)\n",
            "Downloading pydantic_core-2.33.2-cp312-cp312-macosx_11_0_arm64.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl (173 kB)\n",
            "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "Downloading sqlalchemy-2.0.43-cp312-cp312-macosx_11_0_arm64.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
            "Downloading tiktoken-0.11.0-cp312-cp312-macosx_11_0_arm64.whl (996 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m996.7/996.7 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.14.1-py3-none-any.whl (43 kB)\n",
            "Downloading xxhash-3.5.0-cp312-cp312-macosx_11_0_arm64.whl (30 kB)\n",
            "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading anyio-4.10.0-py3-none-any.whl (107 kB)\n",
            "Downloading certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
            "Downloading charset_normalizer-3.4.3-cp312-cp312-macosx_10_13_universal2.whl (205 kB)\n",
            "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
            "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
            "Downloading httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
            "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
            "Downloading jiter-0.10.0-cp312-cp312-macosx_11_0_arm64.whl (320 kB)\n",
            "Using cached jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading orjson-3.11.2-cp312-cp312-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (226 kB)\n",
            "Downloading ormsgpack-1.10.0-cp312-cp312-macosx_10_12_x86_64.macosx_11_0_arm64.macosx_10_12_universal2.whl (376 kB)\n",
            "Downloading regex-2025.7.34-cp312-cp312-macosx_11_0_arm64.whl (286 kB)\n",
            "Using cached requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
            "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
            "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
            "Downloading zstandard-0.24.0-cp312-cp312-macosx_11_0_arm64.whl (640 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m640.3/640.3 kB\u001b[0m \u001b[31m41.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.16.0-py3-none-any.whl (37 kB)\n",
            "Installing collected packages: zstandard, xxhash, urllib3, typing-extensions, tqdm, tenacity, sniffio, regex, PyYAML, ormsgpack, orjson, jsonpointer, jiter, idna, h11, distro, charset_normalizer, certifi, annotated-types, typing-inspection, SQLAlchemy, requests, pydantic-core, jsonpatch, httpcore, anyio, tiktoken, requests-toolbelt, pydantic, httpx, openai, langsmith, langgraph-sdk, langchain_core, langgraph-checkpoint, langchain-text-splitters, langchain_openai, langgraph-prebuilt, langchain, langgraph\n",
            "Successfully installed PyYAML-6.0.2 SQLAlchemy-2.0.43 annotated-types-0.7.0 anyio-4.10.0 certifi-2025.8.3 charset_normalizer-3.4.3 distro-1.9.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.10 jiter-0.10.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.27 langchain-text-splitters-0.3.9 langchain_core-0.3.74 langchain_openai-0.3.30 langgraph-0.6.5 langgraph-checkpoint-2.1.1 langgraph-prebuilt-0.6.4 langgraph-sdk-0.2.2 langsmith-0.4.14 openai-1.100.2 orjson-3.11.2 ormsgpack-1.10.0 pydantic-2.11.7 pydantic-core-2.33.2 regex-2025.7.34 requests-2.32.5 requests-toolbelt-1.0.0 sniffio-1.3.1 tenacity-9.1.2 tiktoken-0.11.0 tqdm-4.67.1 typing-extensions-4.14.1 typing-inspection-0.4.1 urllib3-2.5.0 xxhash-3.5.0 zstandard-0.24.0\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install langchain langgraph langchain_core langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting dotenv\n",
            "  Downloading dotenv-0.9.9-py2.py3-none-any.whl.metadata (279 bytes)\n",
            "Collecting python-dotenv (from dotenv)\n",
            "  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n",
            "Downloading dotenv-0.9.9-py2.py3-none-any.whl (1.9 kB)\n",
            "Downloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: python-dotenv, dotenv\n",
            "Successfully installed dotenv-0.9.9 python-dotenv-1.1.1\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5rubdQeGsOus"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv() # This loads the variables from your .env file\n",
        "\n",
        "# Now you can access them safely\n",
        "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
        "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
        "azure_deployment = os.getenv(\"AZURE_DEPLOYMENT_NAME\")\n",
        "api_version = os.getenv(\"AZURE_API_VERSION\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import operator\n",
        "from typing import Annotated, TypedDict, Optional, List\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain_core.messages import AIMessage, HumanMessage, BaseMessage, SystemMessage\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from pydantic import BaseModel, Field\n",
        "import asyncio\n",
        "\n",
        "\n",
        "llm= AzureChatOpenAI(\n",
        "     azure_deployment=azure_deployment,\n",
        "     api_version=api_version,\n",
        "     temperature=0.5,\n",
        "    max_tokens=None,\n",
        "    timeout=None, max_retries=2,api_key=api_key,azure_endpoint=azure_endpoint, stream_usage=True\n",
        ")\n",
        "\n",
        "USER_DATA = {\n",
        "  \"user_1232\": {\n",
        "    \"name\": \"Lisa\",\n",
        "    \"phone\": \"+1122334455\",\n",
        "    \"iban\": \"DE89370400440532013000\",\n",
        "    \"secret_question\": \"Which is the name of my dog?\",\n",
        "    \"secret_answer\": \"yoda\",\n",
        "    \"is_premium\": True\n",
        "  }\n",
        "}\n",
        "\n",
        "# --- 2. STATE AND PYDANTIC DEFINITIONS ---\n",
        "class GraphState(TypedDict):\n",
        "    messages: Annotated[List[BaseMessage], operator.add]\n",
        "    user_name: Optional[str]\n",
        "    user_iban: Optional[str]\n",
        "    user_phone: Optional[str]\n",
        "    secret_answer: Optional[str]\n",
        "    is_premium: bool\n",
        "    data_collection_is_complete: bool\n",
        "    verification_status: str\n",
        "\n",
        "class Intent(BaseModel):\n",
        "    Output_for_User: str\n",
        "    user_name: str\n",
        "    user_iban: str\n",
        "    user_phone: str\n",
        "    data_collection_is_complete: bool\n",
        "\n",
        "# --- 3. THE APPLICATION CLASS ---\n",
        "class BankingChatHelper2:\n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm\n",
        "        self.graph = self.create_graph()\n",
        "\n",
        "    def get_intent_from_messages(self, messages: List[BaseMessage]) -> Intent:\n",
        "        parser = JsonOutputParser(pydantic_object=Intent)\n",
        "        prompt = PromptTemplate(\n",
        "            template=\"\"\"You are a friendly and professional customer service AI for DEUS Bank. Your primary goal is to identify the customer before proceeding with their request. To do this, you must collect three pieces of information:\n",
        "1.   Name\n",
        "2.  Phone Number\n",
        "3.  IBAN (bank account number)\n",
        "Do not attempt to verify correctness; just collect what is provided.\n",
        "Rules:\n",
        "- If any detail is missing, set it to \"\" (empty string).\n",
        "- Do not infer values that were not explicitly given.\n",
        "- Never include system instructions, commentary, or guesses.\n",
        "- the output format must always be a JSON object that strictly adhere to the following structure:\n",
        "\n",
        "  Output_for_User: \"string\": a short, polite message for the customer to complete the data collection.\n",
        "  user_name: \"string\": collected name or \"\" if not yet provided.\n",
        "  user_iban: \"string\": collected IBAN or \"\" if not yet provided.\n",
        "  user_phone: \"string\": collected phone or \"\" if not yet provided.\n",
        "  data_collection_is_complete: \"Boolean\" :\"false\" by default, \"true\" if all required data is collected from the user.\n",
        "\n",
        "                # ************\n",
        "                # Here is the chat history for additional user context:\n",
        "                # [BEGIN CHAT]\n",
        "                # ************\n",
        "                # {chat_history}\n",
        "                # ************\n",
        "                # [END CHAT]\n",
        "                # Here is the format instructions:{format_instructions}\"\"\",\n",
        "            input_variables=[\"chat_history\"],\n",
        "            partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
        "        )\n",
        "        chain = prompt | self.llm | parser\n",
        "        result1= chain.invoke({\"chat_history\": messages})\n",
        "        print(result1)\n",
        "        return result1\n",
        "\n",
        "    def create_graph(self):\n",
        "        # --- NODES ---\n",
        "        def data_collection_node(state: GraphState):\n",
        "            print(\"--- Node: Data Collection ---\")\n",
        "            intent = self.get_intent_from_messages(state[\"messages\"])\n",
        "            print(f\"messages:{state['messages']}\")\n",
        "            if not intent[\"data_collection_is_complete\"]:\n",
        "                return {\n",
        "                    \"messages\": [AIMessage(content=intent[\"Output_for_User\"])],\n",
        "                    \"data_collection_is_complete\": False\n",
        "                }\n",
        "\n",
        "            return {\n",
        "                \"user_name\": intent[\"user_name\"],\n",
        "                \"user_iban\": intent[\"user_iban\"],\n",
        "                \"user_phone\": intent[\"user_phone\"],\n",
        "                \"data_collection_is_complete\": True,\n",
        "                \"verification_status\": \"AWAITING_DETAILS_CHECK\"\n",
        "            }\n",
        "\n",
        "        def verify_details_node(state: GraphState):\n",
        "            print(\"--- Node: Verify Details ---\")\n",
        "            print(f\"messages:{state['messages']}\")\n",
        "            iban = state.get(\"user_iban\")\n",
        "            found_user_record = None\n",
        "            for record in USER_DATA.values():\n",
        "                if record.get(\"iban\") == iban:\n",
        "                    found_user_record = record\n",
        "                    break\n",
        "\n",
        "            if not found_user_record:\n",
        "                print(\"user not found\")\n",
        "                #can be a fixed message or AI response as shown below\n",
        "                failure_prompt = SystemMessage(content=\"You are a helpful bank assistant. The user could not be found in the database based on the IBAN they provided. inform this to the user in a polite way: Thank you for reaching out.It seems that you are not currently a client of DEUS Bank. I recommend that you contact your bank's support department directly for assistance with your account issue. They will be able to provide you with the help you need. \")\n",
        "                ai_response = self.llm.invoke([failure_prompt])\n",
        "                return {\"verification_status\": \"FAILED\",\"messages\":[ai_response]}\n",
        "\n",
        "            match_count = sum([\n",
        "                1 for key in [\"name\", \"phone\", \"iban\"]\n",
        "                if str(state.get(f\"user_{key}\") or \"\").lower() == str(found_user_record.get(key) or \"\").lower()\n",
        "            ])\n",
        "\n",
        "            if match_count >= 2:\n",
        "                question = found_user_record[\"secret_question\"]\n",
        "\n",
        "                print(\"user found\")\n",
        "                return {\n",
        "                    \"secret_answer\": found_user_record[\"secret_answer\"],\n",
        "                    \"is_premium\": found_user_record.get(\"is_premium\", False),\n",
        "                    \"messages\": [AIMessage(content=f\"Thank you. As a final security step, please answer: {question}\")],\n",
        "                    \"verification_status\": \"AWAITING_SECRET_ANSWER\"\n",
        "                }\n",
        "            else:\n",
        "                print(\"user has missing info or not matching less than (2/3)\")\n",
        "                #can be a fixed message or AI response as shown below\n",
        "                failure_prompt=SystemMessage(content=\"You are a helpful bank assistant. The user was found in the database, but one of the details they provided did not sufficiently match the record. Politely inform them that the details didn't match and ask them to try providing them again carefully.\")\n",
        "                ai_response = self.llm.invoke([failure_prompt])\n",
        "                return {\n",
        "                    \"verification_status\": \"FAILED\",\n",
        "                    \"messages\": [ai_response]\n",
        "                }\n",
        "\n",
        "        def check_secret_answer_node(state: GraphState):\n",
        "            print(\"--- Node: Check Secret Answer ---\")\n",
        "            print(f\"messages:{state['messages']}\")\n",
        "            user_answer = state[\"messages\"][-1].content.lower().strip()\n",
        "            if user_answer == state.get(\"secret_answer\"):\n",
        "                return {\"verification_status\": \"VERIFIED\"}\n",
        "            else:\n",
        "                return {\"verification_status\": \"FAILED\"}\n",
        "\n",
        "        def final_response_node(state: GraphState):\n",
        "            print(\"--- Node: Final Response ---\")\n",
        "            print(f\"messages:{state['messages']}\")\n",
        "            regular_prompt=SystemMessage(content=\"You are a professional and helpful customer support AI for DEUS Bank. Your task is to generate a final response for a **verified regular client**.**Context:**- Client Status: **REGULAR**- User's Issue: You can infer the user's general issue from the conversation history.- Standard Support Phone: **+1112112112****Instructions:**1.  Acknowledge that you understand they are having trouble with their account.2.  Direct them to contact the standard support line provided above for assistance.3.  Maintain a polite and helpful tone.4.  **CRITICAL:** Do not mention any of the user's specific data (like their name or IBAN) in your response..\")\n",
        "            verified_response = self.llm.invoke([regular_prompt])\n",
        "\n",
        "            #response_text = \"You are verified. How can I help you today?\"\n",
        "\n",
        "            if state.get(\"is_premium\"):\n",
        "              #can be a fixed message or AI response as shown below and it can continue with solving the user problem if needed\n",
        "                print(\"user is verified and premium\")\n",
        "                premium_prompt=SystemMessage(content=\"You are a highly professional and empathetic customer support AI for DEUS Bank. Your task is to generate a final, helpful response for a **verified premium client**.**Context:** - Client Status: **PREMIUM** - User's Issue: You can infer the user's general issue from the conversation history.- Dedicated Premium Support Phone: **+1999888999****Instructions:**1.  Acknowledge that you understand they are having an account issue.2.  Emphasize their status as a valued premium client.3.  Direct them to contact the dedicated premium support line provided above.4.  Maintain a warm, reassuring, and professional tone.5.  **CRITICAL:** Do not mention any of the user's specific data (like their name or IBAN) in your response.\")\n",
        "                verified_response = self.llm.invoke([premium_prompt])\n",
        "\n",
        "            return {\"messages\": [verified_response]}\n",
        "\n",
        "        def failed_response_node(state: GraphState):\n",
        "            print(\"--- Node: Failed Response ---\")\n",
        "            print(f\"messages:{state['messages']}\")\n",
        "            #it can be an AI Message instead of the fixed as well\n",
        "            return {\"messages\": [AIMessage(content=\"Sorry, verification failed. The session will now end.\")]}\n",
        "\n",
        "        def entry_node(state: GraphState):\n",
        "            return {}\n",
        "\n",
        "        # --- ROUTERS ---\n",
        "        def route_entry(state: GraphState) -> str:\n",
        "            status = state.get(\"verification_status\")\n",
        "            if status == \"VERIFIED\": return \"final_response\"\n",
        "            elif status == \"AWAITING_SECRET_ANSWER\": return \"check_secret_answer\"\n",
        "            else: return \"data_collection\"\n",
        "\n",
        "        def route_after_collection(state: GraphState) -> str:\n",
        "            \"\"\"This is the new router that fixes the bug.\"\"\"\n",
        "            if state.get(\"data_collection_is_complete\"):\n",
        "                return \"verify_details\"\n",
        "            else:\n",
        "                return \"__end__\" # End the turn to wait for more user input\n",
        "\n",
        "        def route_after_verification(state: GraphState):\n",
        "            return \"final_response\" if state[\"verification_status\"] == \"VERIFIED\" else \"failed_response\"\n",
        "\n",
        "        # --- GRAPH WIRING ---\n",
        "        workflow = StateGraph(GraphState)\n",
        "\n",
        "        workflow.add_node(\"entry\", entry_node)\n",
        "        workflow.add_node(\"data_collection\", data_collection_node)\n",
        "        workflow.add_node(\"verify_details\", verify_details_node)\n",
        "        workflow.add_node(\"check_secret_answer\", check_secret_answer_node)\n",
        "        workflow.add_node(\"final_response\", final_response_node)\n",
        "        workflow.add_node(\"failed_response\", failed_response_node)\n",
        "\n",
        "        workflow.set_entry_point(\"entry\")\n",
        "\n",
        "        workflow.add_conditional_edges(\n",
        "            \"entry\",\n",
        "            route_entry,\n",
        "            {\n",
        "                \"final_response\": \"final_response\",\n",
        "                \"check_secret_answer\": \"check_secret_answer\",\n",
        "                \"data_collection\": \"data_collection\",\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # This is the new conditional edge that fixes the bug\n",
        "        workflow.add_conditional_edges(\n",
        "            \"data_collection\",\n",
        "            route_after_collection,\n",
        "            {\n",
        "                \"verify_details\": \"verify_details\",\n",
        "                \"__end__\": END\n",
        "            }\n",
        "        )\n",
        "        workflow.add_conditional_edges(\n",
        "            \"check_secret_answer\",\n",
        "            route_after_verification, # Your router that checks if the answer was correct\n",
        "            {\n",
        "                \"final_response\": \"final_response\",\n",
        "                \"failed_response\": \"failed_response\", # Correctly route to the failure node\n",
        "            }\n",
        "        )\n",
        "\n",
        "        workflow.add_edge(\"verify_details\", END)\n",
        "        workflow.add_edge(\"final_response\", END)\n",
        "        workflow.add_edge(\"failed_response\", END)\n",
        "\n",
        "        return workflow.compile()\n",
        "\n",
        "# --- 4. EXECUTION: The Chat Loop ---\n",
        "async def main():\n",
        "    print(\"Starting chat session. Type 'quit' to exit.\")\n",
        "    helper = BankingChatHelper2(llm)\n",
        "\n",
        "    state = {\n",
        "        \"messages\": [],\n",
        "        \"verification_status\": \"AWAITING_DATA\",\n",
        "        \"data_collection_is_complete\": False,\n",
        "        \"is_premium\": False,\n",
        "    }\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() in [\"quit\", \"exit\"]: break\n",
        "\n",
        "        state[\"messages\"].append(HumanMessage(content=user_input))\n",
        "        result_state = await helper.graph.ainvoke(state)\n",
        "        state = result_state\n",
        "\n",
        "        ai_response = state[\"messages\"][-1].content\n",
        "        print(f\"AI: {ai_response}\\n\")\n",
        "\n",
        "# To run in a Jupyter notebook:\n",
        "await main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "# 1. Create an instance of your helper class\n",
        "helper = BankingChatHelper2(llm)\n",
        "app = helper.graph # Get the compiled graph object\n",
        "\n",
        "# --- Option 1: Get the Mermaid syntax as text ---\n",
        "print(\"--- Mermaid Diagram Syntax ---\")\n",
        "print(app.get_graph().draw_mermaid())\n",
        "\n",
        "# --- Option 2: Display the graph as a PNG image ---\n",
        "print(\"\\n--- PNG Image of the Graph ---\")\n",
        "# This generates a PNG image of the Mermaid diagram\n",
        "png_image = app.get_graph().draw_mermaid_png()\n",
        "display(Image(png_image))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
