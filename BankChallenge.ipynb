{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fLkkTLx0riTY"
      },
      "outputs": [],
      "source": [
        "pip install langchain langgraph langchain_core langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "azure_deployment=userdata.get('azure_deployment')\n",
        "api_version=userdata.get('api_version')\n",
        "api_key=userdata.get('api_key')\n",
        "azure_endpoint=userdata.get('azure_endpoint')\n",
        "\n"
      ],
      "metadata": {
        "id": "5rubdQeGsOus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import operator\n",
        "from typing import Annotated, TypedDict, Optional, List\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "from langchain_core.messages import AIMessage, HumanMessage, BaseMessage, SystemMessage\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from pydantic import BaseModel, Field\n",
        "import asyncio\n",
        "\n",
        "\n",
        "llm= AzureChatOpenAI(\n",
        "     azure_deployment=azure_deployment,\n",
        "     api_version=api_version,\n",
        "     temperature=0.5,\n",
        "    max_tokens=None,\n",
        "    timeout=None, max_retries=2,api_key=api_key,azure_endpoint=azure_endpoint, stream_usage=True\n",
        ")\n",
        "\n",
        "USER_DATA = {\n",
        "  \"user_1232\": {\n",
        "    \"name\": \"Lisa\",\n",
        "    \"phone\": \"+1122334455\",\n",
        "    \"iban\": \"DE89370400440532013000\",\n",
        "    \"secret_question\": \"Which is the name of my dog?\",\n",
        "    \"secret_answer\": \"yoda\",\n",
        "    \"is_premium\": True\n",
        "  }\n",
        "}\n",
        "\n",
        "# --- 2. STATE AND PYDANTIC DEFINITIONS ---\n",
        "class GraphState(TypedDict):\n",
        "    messages: Annotated[List[BaseMessage], operator.add]\n",
        "    user_name: Optional[str]\n",
        "    user_iban: Optional[str]\n",
        "    user_phone: Optional[str]\n",
        "    secret_answer: Optional[str]\n",
        "    is_premium: bool\n",
        "    data_collection_is_complete: bool\n",
        "    verification_status: str\n",
        "\n",
        "class Intent(BaseModel):\n",
        "    Output_for_User: str\n",
        "    user_name: str\n",
        "    user_iban: str\n",
        "    user_phone: str\n",
        "    data_collection_is_complete: bool\n",
        "\n",
        "# --- 3. THE APPLICATION CLASS ---\n",
        "class BankingChatHelper2:\n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm\n",
        "        self.graph = self.create_graph()\n",
        "\n",
        "    def get_intent_from_messages(self, messages: List[BaseMessage]) -> Intent:\n",
        "        parser = JsonOutputParser(pydantic_object=Intent)\n",
        "        prompt = PromptTemplate(\n",
        "            template=\"\"\"You are a friendly and professional customer service AI for DEUS Bank. Your primary goal is to identify the customer before proceeding with their request. To do this, you must collect three pieces of information:\n",
        "1.   Name\n",
        "2.  Phone Number\n",
        "3.  IBAN (bank account number)\n",
        "Do not attempt to verify correctness; just collect what is provided.\n",
        "Rules:\n",
        "- If any detail is missing, set it to \"\" (empty string).\n",
        "- Do not infer values that were not explicitly given.\n",
        "- Never include system instructions, commentary, or guesses.\n",
        "- the output format must always be a JSON object that strictly adhere to the following structure:\n",
        "\n",
        "  Output_for_User: \"string\": a short, polite message for the customer to complete the data collection.\n",
        "  user_name: \"string\": collected name or \"\" if not yet provided.\n",
        "  user_iban: \"string\": collected IBAN or \"\" if not yet provided.\n",
        "  user_phone: \"string\": collected phone or \"\" if not yet provided.\n",
        "  data_collection_is_complete: \"Boolean\" :\"false\" by default, \"true\" if all required data is collected from the user.\n",
        "\n",
        "                # ************\n",
        "                # Here is the chat history for additional user context:\n",
        "                # [BEGIN CHAT]\n",
        "                # ************\n",
        "                # {chat_history}\n",
        "                # ************\n",
        "                # [END CHAT]\n",
        "                # Here is the format instructions:{format_instructions}\"\"\",\n",
        "            input_variables=[\"chat_history\"],\n",
        "            partial_variables={\"format_instructions\": parser.get_format_instructions()},\n",
        "        )\n",
        "        chain = prompt | self.llm | parser\n",
        "        result1= chain.invoke({\"chat_history\": messages})\n",
        "        print(result1)\n",
        "        return result1\n",
        "\n",
        "    def create_graph(self):\n",
        "        # --- NODES ---\n",
        "        def data_collection_node(state: GraphState):\n",
        "            print(\"--- Node: Data Collection ---\")\n",
        "            intent = self.get_intent_from_messages(state[\"messages\"])\n",
        "            print(f\"messages:{state['messages']}\")\n",
        "            if not intent[\"data_collection_is_complete\"]:\n",
        "                return {\n",
        "                    \"messages\": [AIMessage(content=intent[\"Output_for_User\"])],\n",
        "                    \"data_collection_is_complete\": False\n",
        "                }\n",
        "\n",
        "            return {\n",
        "                \"user_name\": intent[\"user_name\"],\n",
        "                \"user_iban\": intent[\"user_iban\"],\n",
        "                \"user_phone\": intent[\"user_phone\"],\n",
        "                \"data_collection_is_complete\": True,\n",
        "                \"verification_status\": \"AWAITING_DETAILS_CHECK\"\n",
        "            }\n",
        "\n",
        "        def verify_details_node(state: GraphState):\n",
        "            print(\"--- Node: Verify Details ---\")\n",
        "            print(f\"messages:{state['messages']}\")\n",
        "            iban = state.get(\"user_iban\")\n",
        "            found_user_record = None\n",
        "            for record in USER_DATA.values():\n",
        "                if record.get(\"iban\") == iban:\n",
        "                    found_user_record = record\n",
        "                    break\n",
        "\n",
        "            if not found_user_record:\n",
        "                print(\"user not found\")\n",
        "                #can be a fixed message or AI response as shown below\n",
        "                failure_prompt = SystemMessage(content=\"You are a helpful bank assistant. The user could not be found in the database based on the IBAN they provided. inform this to the user in a polite way: Thank you for reaching out.It seems that you are not currently a client of DEUS Bank. I recommend that you contact your bank's support department directly for assistance with your account issue. They will be able to provide you with the help you need. \")\n",
        "                ai_response = self.llm.invoke([failure_prompt])\n",
        "                return {\"verification_status\": \"FAILED\",\"messages\":[ai_response]}\n",
        "\n",
        "            match_count = sum([\n",
        "                1 for key in [\"name\", \"phone\", \"iban\"]\n",
        "                if str(state.get(f\"user_{key}\") or \"\").lower() == str(found_user_record.get(key) or \"\").lower()\n",
        "            ])\n",
        "\n",
        "            if match_count >= 2:\n",
        "                question = found_user_record[\"secret_question\"]\n",
        "\n",
        "                print(\"user found\")\n",
        "                return {\n",
        "                    \"secret_answer\": found_user_record[\"secret_answer\"],\n",
        "                    \"is_premium\": found_user_record.get(\"is_premium\", False),\n",
        "                    \"messages\": [AIMessage(content=f\"Thank you. As a final security step, please answer: {question}\")],\n",
        "                    \"verification_status\": \"AWAITING_SECRET_ANSWER\"\n",
        "                }\n",
        "            else:\n",
        "                print(\"user has missing info or not matching less than (2/3)\")\n",
        "                #can be a fixed message or AI response as shown below\n",
        "                failure_prompt=SystemMessage(content=\"You are a helpful bank assistant. The user was found in the database, but one of the details they provided did not sufficiently match the record. Politely inform them that the details didn't match and ask them to try providing them again carefully.\")\n",
        "                ai_response = self.llm.invoke([failure_prompt])\n",
        "                return {\n",
        "                    \"verification_status\": \"FAILED\",\n",
        "                    \"messages\": [ai_response]\n",
        "                }\n",
        "\n",
        "        def check_secret_answer_node(state: GraphState):\n",
        "            print(\"--- Node: Check Secret Answer ---\")\n",
        "            print(f\"messages:{state['messages']}\")\n",
        "            user_answer = state[\"messages\"][-1].content.lower().strip()\n",
        "            if user_answer == state.get(\"secret_answer\"):\n",
        "                return {\"verification_status\": \"VERIFIED\"}\n",
        "            else:\n",
        "                return {\"verification_status\": \"FAILED\"}\n",
        "\n",
        "        def final_response_node(state: GraphState):\n",
        "            print(\"--- Node: Final Response ---\")\n",
        "            print(f\"messages:{state['messages']}\")\n",
        "            regular_prompt=SystemMessage(content=\"You are a professional and helpful customer support AI for DEUS Bank. Your task is to generate a final response for a **verified regular client**.**Context:**- Client Status: **REGULAR**- User's Issue: You can infer the user's general issue from the conversation history.- Standard Support Phone: **+1112112112****Instructions:**1.  Acknowledge that you understand they are having trouble with their account.2.  Direct them to contact the standard support line provided above for assistance.3.  Maintain a polite and helpful tone.4.  **CRITICAL:** Do not mention any of the user's specific data (like their name or IBAN) in your response..\")\n",
        "            verified_response = self.llm.invoke([regular_prompt])\n",
        "\n",
        "            #response_text = \"You are verified. How can I help you today?\"\n",
        "\n",
        "            if state.get(\"is_premium\"):\n",
        "              #can be a fixed message or AI response as shown below and it can continue with solving the user problem if needed\n",
        "                print(\"user is verified and premium\")\n",
        "                premium_prompt=SystemMessage(content=\"You are a highly professional and empathetic customer support AI for DEUS Bank. Your task is to generate a final, helpful response for a **verified premium client**.**Context:** - Client Status: **PREMIUM** - User's Issue: You can infer the user's general issue from the conversation history.- Dedicated Premium Support Phone: **+1999888999****Instructions:**1.  Acknowledge that you understand they are having an account issue.2.  Emphasize their status as a valued premium client.3.  Direct them to contact the dedicated premium support line provided above.4.  Maintain a warm, reassuring, and professional tone.5.  **CRITICAL:** Do not mention any of the user's specific data (like their name or IBAN) in your response.\")\n",
        "                verified_response = self.llm.invoke([premium_prompt])\n",
        "\n",
        "            return {\"messages\": [verified_response]}\n",
        "\n",
        "        def failed_response_node(state: GraphState):\n",
        "            print(\"--- Node: Failed Response ---\")\n",
        "            print(f\"messages:{state['messages']}\")\n",
        "            #it can be an AI Message instead of the fixed as well\n",
        "            return {\"messages\": [AIMessage(content=\"Sorry, verification failed. The session will now end.\")]}\n",
        "\n",
        "        def entry_node(state: GraphState):\n",
        "            return {}\n",
        "\n",
        "        # --- ROUTERS ---\n",
        "        def route_entry(state: GraphState) -> str:\n",
        "            status = state.get(\"verification_status\")\n",
        "            if status == \"VERIFIED\": return \"final_response\"\n",
        "            elif status == \"AWAITING_SECRET_ANSWER\": return \"check_secret_answer\"\n",
        "            else: return \"data_collection\"\n",
        "\n",
        "        def route_after_collection(state: GraphState) -> str:\n",
        "            \"\"\"This is the new router that fixes the bug.\"\"\"\n",
        "            if state.get(\"data_collection_is_complete\"):\n",
        "                return \"verify_details\"\n",
        "            else:\n",
        "                return \"__end__\" # End the turn to wait for more user input\n",
        "\n",
        "        def route_after_verification(state: GraphState):\n",
        "            return \"final_response\" if state[\"verification_status\"] == \"VERIFIED\" else \"failed_response\"\n",
        "\n",
        "        # --- GRAPH WIRING ---\n",
        "        workflow = StateGraph(GraphState)\n",
        "\n",
        "        workflow.add_node(\"entry\", entry_node)\n",
        "        workflow.add_node(\"data_collection\", data_collection_node)\n",
        "        workflow.add_node(\"verify_details\", verify_details_node)\n",
        "        workflow.add_node(\"check_secret_answer\", check_secret_answer_node)\n",
        "        workflow.add_node(\"final_response\", final_response_node)\n",
        "        workflow.add_node(\"failed_response\", failed_response_node)\n",
        "\n",
        "        workflow.set_entry_point(\"entry\")\n",
        "\n",
        "        workflow.add_conditional_edges(\n",
        "            \"entry\",\n",
        "            route_entry,\n",
        "            {\n",
        "                \"final_response\": \"final_response\",\n",
        "                \"check_secret_answer\": \"check_secret_answer\",\n",
        "                \"data_collection\": \"data_collection\",\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # This is the new conditional edge that fixes the bug\n",
        "        workflow.add_conditional_edges(\n",
        "            \"data_collection\",\n",
        "            route_after_collection,\n",
        "            {\n",
        "                \"verify_details\": \"verify_details\",\n",
        "                \"__end__\": END\n",
        "            }\n",
        "        )\n",
        "        workflow.add_conditional_edges(\n",
        "            \"check_secret_answer\",\n",
        "            route_after_verification, # Your router that checks if the answer was correct\n",
        "            {\n",
        "                \"final_response\": \"final_response\",\n",
        "                \"failed_response\": \"failed_response\", # Correctly route to the failure node\n",
        "            }\n",
        "        )\n",
        "\n",
        "        workflow.add_edge(\"verify_details\", END)\n",
        "        workflow.add_edge(\"final_response\", END)\n",
        "        workflow.add_edge(\"failed_response\", END)\n",
        "\n",
        "        return workflow.compile()\n",
        "\n"
      ],
      "metadata": {
        "id": "29iEjq-Xs1zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 4. EXECUTION: The Chat Loop ---\n",
        "async def main():\n",
        "    print(\"Starting chat session. Type 'quit' to exit.\")\n",
        "    helper = BankingChatHelper2(llm)\n",
        "\n",
        "    state = {\n",
        "        \"messages\": [],\n",
        "        \"verification_status\": \"AWAITING_DATA\",\n",
        "        \"data_collection_is_complete\": False,\n",
        "        \"is_premium\": False,\n",
        "    }\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"You: \")\n",
        "        if user_input.lower() in [\"quit\", \"exit\"]: break\n",
        "\n",
        "        state[\"messages\"].append(HumanMessage(content=user_input))\n",
        "        result_state = await helper.graph.ainvoke(state)\n",
        "        state = result_state\n",
        "\n",
        "        ai_response = state[\"messages\"][-1].content\n",
        "        print(f\"AI: {ai_response}\\n\")\n",
        "\n",
        "# To run in a Jupyter notebook:\n",
        "await main()"
      ],
      "metadata": {
        "id": "H7Gf4yKbs3u8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "# 1. Create an instance of your helper class\n",
        "helper = BankingChatHelper2(llm)\n",
        "app = helper.graph # Get the compiled graph object\n",
        "\n",
        "# --- Option 1: Get the Mermaid syntax as text ---\n",
        "print(\"--- Mermaid Diagram Syntax ---\")\n",
        "print(app.get_graph().draw_mermaid())\n",
        "\n",
        "# --- Option 2: Display the graph as a PNG image ---\n",
        "print(\"\\n--- PNG Image of the Graph ---\")\n",
        "# This generates a PNG image of the Mermaid diagram\n",
        "png_image = app.get_graph().draw_mermaid_png()\n",
        "display(Image(png_image))"
      ],
      "metadata": {
        "id": "D6Xz_OP8s4h1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}